{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusteren\n",
    "\n",
    "Als eerste poging is geprobeerd om de data te verdelen in clusters, om te kijken of je zo de wolken zou kunnen onderscheiden van de vogels. Hiervoor zijn verschillende clustering algoritmen gebruikt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K - means\n",
    "\n",
    "K - means is een simpel partitie algoritme die de variante in een cluster probeert te verlagen.\n",
    "Resultaten:\n",
    "- Als je de xyz waarden meegeeft als variabele om op te clusteren, komen er nogal scherpe lijnen tussen de clusters die niet overeenkomen met vogels en wolken. Dit kan te maken hebben met dat k-means de variantie probeerd te minimaliseren, en omdat de variantie van een vogelgroep per definitie groot is, dit nooit goed geclusterd zal worden.\n",
    "- Als je alleen op DBZH waarde clustert worden de vogels wel gedetecteerd, maar zitten er ook veel randjes van regenwolken bij die ook een lage DBZH waarde hebben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import sklearn.cluster as cluster\n",
    "import shutil as su\n",
    "from sklearn import metrics\n",
    "import hdbscan\n",
    "\n",
    "def kmeans(path_and_name):\n",
    "    original = np.genfromtxt(path_and_name, delimiter=',')\n",
    "    headless = original[1:]\n",
    "    data = headless[:, 0]\n",
    "    data = data.reshape(-1,1)\n",
    "    print(data)\n",
    "    ks = KMeans(n_clusters=3).fit(data)\n",
    "\n",
    "    print(\"Labels for kmeans: \")\n",
    "    #labels = ks.labels_\n",
    "    labels = np.append(\"cluster\", labels)\n",
    "    print(labels)\n",
    "\n",
    "    newdata = np.append(np.reshape(headless[:,3], (-1,1)), np.reshape(headless[:,4], (-1,1)), axis=1)\n",
    "    newdata = np.append(newdata, np.reshape(headless[:,5], (-1,1)), axis=1)\n",
    "    newdata = np.append(newdata, np.reshape(labels,(-1,1)), axis=1)\n",
    "\n",
    "    np.savetxt(\"kmeans.csv\", newdata, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN\n",
    "\n",
    "DBSCAN is een clustering algoritme dat probeert clusters te vinden met een hoge dichtheid. Niet alle punten hoeven bij een cluster te horen. Dit zou in theorie dus de wolken kunnen vinden als dichte clusters, en de vogels overhouden als ongeclusterd. \n",
    "Resultaten:\n",
    "\n",
    "- DBHSCAN heeft een niet intuitieve parameter eps, voor elke scan anders is. Met sommige waarden is er totaal geen resultaat, voor sommigen iets beter.\n",
    "- Voor de data van 01 10 2016 om 5:00 worden met eps=3000 de wolken aardig weggefilterd. Er blijft wat ruis over en wat vogels worden tot een wolk gerekend, maar het lijkt alsof je nog iets kan zien waar een wolk heeft gezeten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DBSCAN(filename):\n",
    "    original = np.genfromtxt(filename, delimiter=',')\n",
    "    headless = original[1:]\n",
    "    DB = cluster.DBSCAN(eps = 500000).fit(headless)\n",
    "    print (len(set(DB.labels_)))\n",
    "    print (percentage_empty(DB.labels_))\n",
    "    width = len(headless[0])\n",
    "    length = len(headless)\n",
    "    b = np.zeros((length,width+1))\n",
    "    b[:,:-1] = headless\n",
    "    b[:, width] = np.array(DB.labels_)\n",
    "    np.savetxt(\"output_csv\\DBSCAN.csv\", b, delimiter=',')\n",
    "    print (\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDBSCAN\n",
    "\n",
    "HDBSCAN zou een nog geavanceerde versie zijn van HDBSCAN, met een meer intuitieve parameter: min_cluster_size. In ons geval levert dit nog geen betere resultaten op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def HDBSCAN(filename):\n",
    "    original = np.genfromtxt(filename, delimiter=',')\n",
    "    headless = original[1:]\n",
    "    DB = hdbscan.HDBSCAN(min_cluster_size = 30).fit(headless)\n",
    "    print (len(set(DB.labels_)))\n",
    "    print (percentage_empty(DB.labels_))\n",
    "    width = len(headless[0])\n",
    "    length = len(headless)\n",
    "    b = np.zeros((length,width+1))\n",
    "    b[:,:-1] = headless\n",
    "    b[:, width] = np.array(DB.labels_)\n",
    "    np.savetxt(\"output_csv\\HDBSCAN.csv\", b, delimiter=',')\n",
    "    print (\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pogingen tot supervised learing\n",
    "\n",
    "De gegeven data is niet gelabeld, waardoor je niet echt supervised algoritmes kunt gebruiken. Wel kunnen we een poging om een intuitive labeling van een clustering algoritme of van het huidige foldtobirds algoritme te gebruiken om op andere plaatjes toe te passen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knn met omliggende punten\n",
    "omdat een belangrijk deel van de informatie van een datapunt ligt in de informatie van de datapunten er omheen (vogels zijn verder uit elkaar dan wolken, verschillen meer in richting etc.) is geprobeerd om van elk punt de dichtbijzijnde tien punten te vinden en de varantie in deze punten uit te rekenen. Op deze data is het kNN algoritme toegepast\n",
    "Resultaten:\n",
    "- het vinden van de dichtbijzijnde punten duurde erg lang (kan nog beter met python implementatie) daardoor konden niet alle punten gebruikt worden\n",
    "- Met een tiende van de punten was het wel te doen, alleen gaf het toepassen van van kNN algortime geen nuttige informatie. Alleen bij k=2 kwamen er uberhoupt vogels uit, en als je die visualiseerd in cloudcompare waren het grotendeels niet de juiste punten. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# KAN DUS OOK BETER MET PYTHON VERSIE\n",
    "def find_n_closest_points(point, data, n):\n",
    "    min_points = data[0:n] #zoiets\n",
    "    min_distances = []\n",
    "    for min_point in min_points:\n",
    "        min_distances.append(np.linalg.norm(point - min_point))\n",
    "\n",
    "    for line in range(len(data)):\n",
    "        thresh = max(min_distances)    \n",
    "        thresh_index = min_distances.index(thresh)\n",
    "        dist = abs(np.linalg.norm(point - data[line]))\n",
    "        if dist < thresh and (data[line] not in min_points):\n",
    "            min_points[thresh_index] = data[line]\n",
    "            min_distances[thresh_index] = dist        \n",
    "    return min_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# neemt een csv file, vind voor elk punt de dichtbijzijne punten, en de varantie tussen die punten voor elke varibele\n",
    "# output dit in een nieuwe csv file\n",
    "def csv_with_varances(filename):\n",
    "    original = np.genfromtxt(filename, delimiter=',')\n",
    "    headless = original[1:]\n",
    "    lenh = len(headless)\n",
    "    idx = np.random.randint(lenh, size=10000)\n",
    "    headless = headless[idx,:]\n",
    "    total = []\n",
    "    count = 0\n",
    "    for points in headless:\n",
    "        count = count + 1\n",
    "        closest = find_n_closest_points(points, headless, 10)\n",
    "        var = np.var(closest, axis=0)\n",
    "        total.append(np.concatenate((points, var)))\n",
    "    np.savetxt(\"output_csv\\csv_met_variances.csv\", total, delimiter=',')\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knn zonder neighbours\n",
    "Zonder de dichtbijzijnde punten heb kun je eigenlijk alleen de DBZH waarden gebruiken om te clusteren, maar mogelijk zou je ook de xyz locatie van een punt kunnen gebruiken als je twee opeenvolgende beelden gebruikt. Resultaten:\n",
    "- Niet echt goed, best wel chunky en verschillende hoogsten worden amper los geclassificeerd. Dit is nu geprobeerd met de DBSCAN van 1 oktober 5:00 als training data, en 5:15 als testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LET OP\n",
    "# Deze functies zijn nogal specifiek per file, wat ze doen is de nuttige dingen er uit filteren, en eventueel de \n",
    "# classificatie wat opschonen. Dit kan natuurlijk beter in een functie waar je aangeeft wele kolommen je wil, zal ik nog even fixen\n",
    "def clean(filename):\n",
    "    data = np.genfromtxt(filename, delimiter=',')\n",
    "    useful_columns = data[:, [0, 7, 9, 10, 11, 12 ]]\n",
    "    labels = data[:, 6]\n",
    "    for cluster_number in range(len(labels)):\n",
    "        if labels[cluster_number] == -1:\n",
    "            labels[cluster_number] = 0\n",
    "        else:\n",
    "            labels[cluster_number] = 1\n",
    "    count = 0\n",
    "    for i in labels:\n",
    "        if i == 0:\n",
    "            count = count + 1\n",
    "    print (count)\n",
    "        \n",
    "    return [useful_columns, labels]\n",
    "\n",
    "def clean_whitout_neighbours(filename, labeled = 0):\n",
    "    data = np.genfromtxt(filename, delimiter=',')\n",
    "    if labeled == 1:\n",
    "        wh = data[1:]\n",
    "        return wh[:, [0, 1, 2, 3, 4]]\n",
    "    useful_columns = data[:, [0, 2, 3, 4, 5]]\n",
    "    labels = data[:, 6]\n",
    "    for cluster_number in range(len(labels)):\n",
    "        if labels[cluster_number] == -1:\n",
    "            labels[cluster_number] = 0\n",
    "        else:\n",
    "            labels[cluster_number] = 1\n",
    "    return [useful_columns, labels]\n",
    "\n",
    "    \n",
    "\n",
    "def clean_unlabeled(filename):\n",
    "    data = np.genfromtxt(filename, delimiter=',')\n",
    "    useful_columns = data[:, [0, 6, 8, 9, 10, 11 ]]\n",
    "    return useful_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# deze functie voegt de labels toe aan een csvfile, ook nogal specifiek per file, kan beter\n",
    "def csv_output(filename, output):\n",
    "    ori = np.genfromtxt(filename, delimiter=',')\n",
    "    data = ori[1:]\n",
    "    useful_columns = data[:, [2, 3, 4]]\n",
    "    print (len(useful_columns))\n",
    "    print (len(output))\n",
    "    total = np.append(useful_columns, np.reshape(output,(-1,1)), axis = 1)\n",
    "    np.savetxt(\"output_csv\\knnshit2.csv\", total, delimiter=',')\n",
    "    print (total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=1)\n",
    "neigh.fit(training[0], training[1]) \n",
    "output_labels = neigh.predict(testing)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
