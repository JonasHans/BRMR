{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusteren\n",
    "\n",
    "Als eerste poging is geprobeerd om de data te verdelen in clusters, om te kijken of je zo de wolken zou kunnen onderscheiden van de vogels. Hiervoor zijn verschillende clustering algoritmen gebruikt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K - means\n",
    "\n",
    "K - means is een simpel partitie algoritme die de variante in een cluster probeert te verlagen.\n",
    "Resultaten:\n",
    "- Als je de xyz waarden meegeeft als variabele om op te clusteren, komen er nogal scherpe lijnen tussen de clusters die niet overeenkomen met vogels en wolken. Dit kan te maken hebben met dat k-means de variantie probeerd te minimaliseren, en omdat de variantie van een vogelgroep per definitie groot is, dit nooit goed geclusterd zal worden.\n",
    "- Als je alleen op DBZH waarde clustert worden de vogels wel gedetecteerd, maar zitten er ook veel randjes van regenwolken bij die ook een lage DBZH waarde hebben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import sklearn.cluster as cluster\n",
    "import shutil as su\n",
    "from sklearn import metrics\n",
    "#import hdbscan\n",
    "\n",
    "def kmeans(path_and_name):\n",
    "    original = np.genfromtxt(path_and_name, delimiter=',')\n",
    "    headless = original[1:]\n",
    "    data = headless[:, 0]\n",
    "    data = data.reshape(-1,1)\n",
    "    print(data)\n",
    "    ks = KMeans(n_clusters=3).fit(data)\n",
    "\n",
    "    print(\"Labels for kmeans: \")\n",
    "    #labels = ks.labels_\n",
    "    labels = np.append(\"cluster\", labels)\n",
    "    print(labels)\n",
    "\n",
    "    newdata = np.append(np.reshape(headless[:,3], (-1,1)), np.reshape(headless[:,4], (-1,1)), axis=1)\n",
    "    newdata = np.append(newdata, np.reshape(headless[:,5], (-1,1)), axis=1)\n",
    "    newdata = np.append(newdata, np.reshape(labels,(-1,1)), axis=1)\n",
    "\n",
    "    np.savetxt(\"kmeans.csv\", newdata, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN\n",
    "\n",
    "DBSCAN is een clustering algoritme dat probeert clusters te vinden met een hoge dichtheid. Niet alle punten hoeven bij een cluster te horen. Dit zou in theorie dus de wolken kunnen vinden als dichte clusters, en de vogels overhouden als ongeclusterd. \n",
    "Resultaten:\n",
    "\n",
    "- DBHSCAN heeft een niet intuitieve parameter eps, voor elke scan anders is. Met sommige waarden is er totaal geen resultaat, voor sommigen iets beter.\n",
    "- Voor de data van 01 10 2016 om 5:00 worden met eps=3000 de wolken aardig weggefilterd. Er blijft wat ruis over en wat vogels worden tot een wolk gerekend, maar het lijkt alsof je nog iets kan zien waar een wolk heeft gezeten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-1655e09f0a45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"output_csv\\DBSCAN.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mDBSCAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'csvData/160930/160930-00-00.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-1655e09f0a45>\u001b[0m in \u001b[0;36mDBSCAN\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0moriginal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mheadless\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mDB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSpectralClustering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheadless\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpercentage_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/thomas/anaconda3/lib/python2.7/site-packages/sklearn/cluster/spectral.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    461\u001b[0m             self.affinity_matrix_ = pairwise_kernels(X, metric=self.affinity,\n\u001b[1;32m    462\u001b[0m                                                      \u001b[0mfilter_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m                                                      **params)\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/thomas/anaconda3/lib/python2.7/site-packages/sklearn/metrics/pairwise.pyc\u001b[0m in \u001b[0;36mpairwise_kernels\u001b[0;34m(X, Y, metric, filter_params, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1403\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown kernel %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1405\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/thomas/anaconda3/lib/python2.7/site-packages/sklearn/metrics/pairwise.pyc\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[0;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1088\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0;31m# Special case to avoid picklability checks in delayed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0;31m# TODO: in some cases, backend='threading' may be appropriate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/thomas/anaconda3/lib/python2.7/site-packages/sklearn/metrics/pairwise.pyc\u001b[0m in \u001b[0;36mrbf_kernel\u001b[0;34m(X, Y, gamma)\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m     \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meuclidean_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m     \u001b[0mK\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# exponentiate K in-place\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/thomas/anaconda3/lib/python2.7/site-packages/sklearn/metrics/pairwise.pyc\u001b[0m in \u001b[0;36meuclidean_distances\u001b[0;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0mYY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m     \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m     \u001b[0mdistances\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0mdistances\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/thomas/anaconda3/lib/python2.7/site-packages/sklearn/utils/extmath.pyc\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def DBSCAN(filename):\n",
    "    original = np.genfromtxt(filename, delimiter=',')\n",
    "    headless = original[1:]\n",
    "    DB = cluster.SpectralClustering(8).fit(headless)\n",
    "    print (len(set(DB.labels_)))\n",
    "    print (percentage_empty(DB.labels_))\n",
    "    width = len(headless[0])\n",
    "    length = len(headless)\n",
    "    b = np.zeros((length,width+1))\n",
    "    b[:,:-1] = headless\n",
    "    b[:, width] = np.array(DB.labels_)\n",
    "    np.savetxt(\"output_csv\\DBSCAN.csv\", b, delimiter=',')\n",
    "    print (\"done\")\n",
    "DBSCAN('csvData/160930/160930-00-00.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 37.5  37.5  37.  ...,  -6.   -6.   -6.5]\n",
      "[ 37.5  37.5  37.  ...,  -6.   -6.   -6.5]\n",
      "(768, 1024)\n",
      "(786432, 1)\n",
      "Compute structured hierarchical clustering...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-2ae34b73f9dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mSC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'csvData/160930/160930-00-00.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-2ae34b73f9dd>\u001b[0m in \u001b[0;36mSC\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     38\u001b[0m     ward = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward',\n\u001b[1;32m     39\u001b[0m                                    connectivity=connectivity)\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Elapsed time: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/thomas/anaconda3/lib/python2.7/site-packages/sklearn/cluster/hierarchical.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    740\u001b[0m             memory.cache(tree_builder)(X, connectivity,\n\u001b[1;32m    741\u001b[0m                                        \u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m                                        **kwargs)\n\u001b[0m\u001b[1;32m    743\u001b[0m         \u001b[0;31m# Cut the tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcompute_full_tree\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/thomas/anaconda3/lib/python2.7/site-packages/sklearn/externals/joblib/memory.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/thomas/anaconda3/lib/python2.7/site-packages/sklearn/cluster/hierarchical.pyc\u001b[0m in \u001b[0;36mward_tree\u001b[0;34m(X, connectivity, n_clusters, return_distance)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0minert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheappop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minertia\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mused_node\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mused_node\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0mparent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time as time\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.image import grid_to_graph\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# #############################################################################\n",
    "# Generate data\n",
    "def SC(filename):\n",
    "    try:  # SciPy >= 0.16 have face in misc\n",
    "        from scipy.misc import face\n",
    "        face = face(gray=True)\n",
    "    except ImportError:\n",
    "        face = sp.face(gray=True)\n",
    "    original = np.genfromtxt(filename, delimiter=',')\n",
    "    headless = original[1:]\n",
    "    dBZ = headless[:,0]\n",
    "    XYZ = headless[:, [3,4,5]]\n",
    "    X = np.zeros((len(dBZ), 4))\n",
    "    X[:,0] = dBZ\n",
    "    print(dBZ)\n",
    "    X[:, 1] = XYZ[:,0]\n",
    "    X[:, 2] = XYZ[:,1]\n",
    "    X[:, 3] = XYZ[:,2]\n",
    "    print(X[:,0])\n",
    "    newface = np.reshape(face, (-1,1))\n",
    "    print(face.shape)\n",
    "    print(newface.shape)\n",
    "    newX = np.reshape(X, (-1, 1))\n",
    "    connectivity = grid_to_graph(*X.shape)\n",
    "    print(\"Compute structured hierarchical clustering...\")\n",
    "    st = time.time()\n",
    "    n_clusters = 15  # number of regions\n",
    "    ward = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward',\n",
    "                                   connectivity=connectivity)\n",
    "    ward.fit(newX)\n",
    "    label = np.reshape(ward.labels_, X.shape)\n",
    "    print(\"Elapsed time: \", time.time() - st)\n",
    "    print(\"Number of pixels: \", label.size)\n",
    "    print(\"Number of clusters: \", np.unique(label).size)\n",
    "\n",
    "    # #############################################################################\n",
    "    # Plot the results on an image\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(X, cmap=plt.cm.gray)\n",
    "    for l in range(n_clusters):\n",
    "        plt.contour(label == l, contours=1,\n",
    "                colors=[plt.cm.spectral(l / float(n_clusters)), ])\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#SC('csvData/160930/160930-00-00.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDBSCAN\n",
    "\n",
    "HDBSCAN zou een nog geavanceerde versie zijn van HDBSCAN, met een meer intuitieve parameter: min_cluster_size. In ons geval levert dit nog geen betere resultaten op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def HDBSCAN(filename):\n",
    "    original = np.genfromtxt(filename, delimiter=',')\n",
    "    headless = original[1:]\n",
    "    DB = hdbscan.HDBSCAN(min_cluster_size = 30).fit(headless)\n",
    "    print (len(set(DB.labels_)))\n",
    "    print (percentage_empty(DB.labels_))\n",
    "    width = len(headless[0])\n",
    "    length = len(headless)\n",
    "    b = np.zeros((length,width+1))\n",
    "    b[:,:-1] = headless\n",
    "    b[:, width] = np.array(DB.labels_)\n",
    "    np.savetxt(\"output_csv\\HDBSCAN.csv\", b, delimiter=',')\n",
    "    print (\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pogingen tot supervised learing\n",
    "\n",
    "De gegeven data is niet gelabeld, waardoor je niet echt supervised algoritmes kunt gebruiken. Wel kunnen we een poging om een intuitive labeling van een clustering algoritme of van het huidige foldtobirds algoritme te gebruiken om op andere plaatjes toe te passen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knn met omliggende punten\n",
    "omdat een belangrijk deel van de informatie van een datapunt ligt in de informatie van de datapunten er omheen (vogels zijn verder uit elkaar dan wolken, verschillen meer in richting etc.) is geprobeerd om van elk punt de dichtbijzijnde tien punten te vinden en de varantie in deze punten uit te rekenen. Op deze data is het kNN algoritme toegepast\n",
    "Resultaten:\n",
    "- het vinden van de dichtbijzijnde punten duurde erg lang (kan nog beter met python implementatie) daardoor konden niet alle punten gebruikt worden\n",
    "- Met een tiende van de punten was het wel te doen, alleen gaf het toepassen van van kNN algortime geen nuttige informatie. Alleen bij k=2 kwamen er uberhoupt vogels uit, en als je die visualiseerd in cloudcompare waren het grotendeels niet de juiste punten. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# KAN DUS OOK BETER MET PYTHON VERSIE\n",
    "def find_n_closest_points(point, data, n):\n",
    "    min_points = data[0:n] #zoiets\n",
    "    min_distances = []\n",
    "    for min_point in min_points:\n",
    "        min_distances.append(np.linalg.norm(point - min_point))\n",
    "\n",
    "    for line in range(len(data)):\n",
    "        thresh = max(min_distances)    \n",
    "        thresh_index = min_distances.index(thresh)\n",
    "        dist = abs(np.linalg.norm(point - data[line]))\n",
    "        if dist < thresh and (data[line] not in min_points):\n",
    "            min_points[thresh_index] = data[line]\n",
    "            min_distances[thresh_index] = dist        \n",
    "    return min_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# neemt een csv file, vind voor elk punt de dichtbijzijne punten, en de varantie tussen die punten voor elke varibele\n",
    "# output dit in een nieuwe csv file\n",
    "def csv_with_varances(filename):\n",
    "    original = np.genfromtxt(filename, delimiter=',')\n",
    "    headless = original[1:]\n",
    "    lenh = len(headless)\n",
    "    idx = np.random.randint(lenh, size=10000)\n",
    "    headless = headless[idx,:]\n",
    "    total = []\n",
    "    count = 0\n",
    "    for points in headless:\n",
    "        count = count + 1\n",
    "        closest = find_n_closest_points(points, headless, 10)\n",
    "        var = np.var(closest, axis=0)\n",
    "        total.append(np.concatenate((points, var)))\n",
    "    np.savetxt(\"output_csv\\csv_met_variances.csv\", total, delimiter=',')\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knn zonder neighbours\n",
    "Zonder de dichtbijzijnde punten heb kun je eigenlijk alleen de DBZH waarden gebruiken om te clusteren, maar mogelijk zou je ook de xyz locatie van een punt kunnen gebruiken als je twee opeenvolgende beelden gebruikt. Resultaten:\n",
    "- Niet echt goed, best wel chunky en verschillende hoogsten worden amper los geclassificeerd. Dit is nu geprobeerd met de DBSCAN van 1 oktober 5:00 als training data, en 5:15 als testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LET OP\n",
    "# Deze functies zijn nogal specifiek per file, wat ze doen is de nuttige dingen er uit filteren, en eventueel de \n",
    "# classificatie wat opschonen. Dit kan natuurlijk beter in een functie waar je aangeeft wele kolommen je wil, zal ik nog even fixen\n",
    "def clean(filename):\n",
    "    data = np.genfromtxt(filename, delimiter=',')\n",
    "    useful_columns = data[:, [0, 7, 9, 10, 11, 12 ]]\n",
    "    labels = data[:, 6]\n",
    "    for cluster_number in range(len(labels)):\n",
    "        if labels[cluster_number] == -1:\n",
    "            labels[cluster_number] = 0\n",
    "        else:\n",
    "            labels[cluster_number] = 1\n",
    "    count = 0\n",
    "    for i in labels:\n",
    "        if i == 0:\n",
    "            count = count + 1\n",
    "    print (count)\n",
    "        \n",
    "    return [useful_columns, labels]\n",
    "\n",
    "def clean_whitout_neighbours(filename, labeled = 0):\n",
    "    data = np.genfromtxt(filename, delimiter=',')\n",
    "    if labeled == 1:\n",
    "        wh = data[1:]\n",
    "        return wh[:, [0, 1, 2, 3, 4]]\n",
    "    useful_columns = data[:, [0, 2, 3, 4, 5]]\n",
    "    labels = data[:, 6]\n",
    "    for cluster_number in range(len(labels)):\n",
    "        if labels[cluster_number] == -1:\n",
    "            labels[cluster_number] = 0\n",
    "        else:\n",
    "            labels[cluster_number] = 1\n",
    "    return [useful_columns, labels]\n",
    "\n",
    "    \n",
    "\n",
    "def clean_unlabeled(filename):\n",
    "    data = np.genfromtxt(filename, delimiter=',')\n",
    "    useful_columns = data[:, [0, 6, 8, 9, 10, 11 ]]\n",
    "    return useful_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# deze functie voegt de labels toe aan een csvfile, ook nogal specifiek per file, kan beter\n",
    "def csv_output(filename, output):\n",
    "    ori = np.genfromtxt(filename, delimiter=',')\n",
    "    data = ori[1:]\n",
    "    useful_columns = data[:, [2, 3, 4]]\n",
    "    print (len(useful_columns))\n",
    "    print (len(output))\n",
    "    total = np.append(useful_columns, np.reshape(output,(-1,1)), axis = 1)\n",
    "    np.savetxt(\"output_csv\\knnshit2.csv\", total, delimiter=',')\n",
    "    print (total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=1)\n",
    "neigh.fit(training[0], training[1]) \n",
    "output_labels = neigh.predict(testing)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
